# Chapter 8. Deep Learning for Time Series Prediction I
- `딥러닝(Deep Learning)`은 머신러닝보다 약간 더 복잡하고 세부적인 분야
  - 머신러닝과 딥러닝은 모두 **데이터 과학**(Data Science)에 속한다.
- `딥러닝`은 대부분 **신경망(Neural Networks)에 관한 것**
  - `신경망`은 **매우 정교하고 강력한 알고리즘**
  - `신경망`은 **복잡하고 비선형적인 변수 간 관계도 잘 포착**할 수 있는 **강력한 도구**
## A Walk Through Neural Networks
- `인공신경망`(**ANN**: Artificial Neural Network)은 `신경과학`(Neurology) 연구에서 시작
  - `신경과학` 에서 인간의 뇌와 복잡하게 연결된 뉴런 네트워크가 어떻게 작동하는지를 이해하고자 했음
  - 이를 바탕으로 생물학적 신경망을 `컴퓨터적으로 모방`한 모델이 **인공신경망**

#### 🧠 인공신경망의 구조
- `인공신경망(ANN)`은 **노드**(=인공 뉴런)들이 **계층적으로 연결된 구조**
- 입력층 → 은닉층(1개 이상) → 출력층
  1. **입력층 (Input Layer)**
  - `데이터가 처음 입력되는 지점`
    - 수치형, 범주형, 센서 데이터 등 다양한 입력 가능

  2. **은닉층 (Hidden Layers)**
  - `입력 데이터를 처리`하는 계층 (1개 이상 가능)
  - 각 뉴런은 입력값을 받아 계산을 수행하고, `결과를 다음 계층에 전달`

  3. **출력층 (Output Layer)**
  - `마지막 예측 결과를 생성`
    - 문제 유형(분류/회귀 등)에 따라 뉴런의 개수와 출력 형태가 달라짐

- ![img.png](images/ch08/img.png)
- **인공신경망**(ANN: Artificial Neural Network)의 간단한 예시
  - 2개의 입력 노드에서 시작, **4개의 은닉층**(hidden layers)을 거쳐 `계산`이 이루어짐
  - 최종적으로 **출력층**(output layer)에서 **가중 예측**(weighted prediction)을 출력

#### 🧠 각 뉴런(Neuron)의 주요 역할
- **가중 입력 계산**
  - `이전 층 또는 입력 데이터`로 부터 **입력값**(input)을 받음
  - 각 입력값은 **가중치(weight)와 곱해짐**
    - `가중치`는 해당 입력의 **중요도 또는 연결 강도를 의미**
    - 모든 가중 입력을 **합산**(sum)함
      - 은닉층의 각 뉴런이 이전 층의 모든 뉴런으로부터 입력을 받아서 그것들을 하나로 모아 계산
- **활성화 함수 적용 (Activation Function)**
  - 가중 합을 기반으로 비선형 함수를 적용함 
  - 이 과정을 통해 뉴런의 **출력값**(output)이 결정됨 
  - `활성화 함수`는 모델이 **비선형 패턴을 학습할 수 있게 해줌**

#### 🔁 학습 과정 (Training)
- `신경망`은 학습 중에 **가중치를 조정함으로써 성능을 향상**
  - 일반적으로 `경사하강법(gradient descent)` 같은 최적화 알고리즘을 사용
- **손실 함수**(loss function)를 통해 `모델의 예측 오류를 계산`
  - 손실 함수의 **기울기**(gradient)를 계산하여, **가중치를 오차를 줄이는 방향으로 업데이트**

> Note
> - Forward Propagation (순전파) 
>   - 입력 데이터를 입력층에 넣고 
>   - 각 은닉층을 거치며 가중치 × 입력값 + 편향 → 활성화 함수를 통해 계산
>   - 예측 결과를 출력하는 전체 흐름
> - Backpropagation (역전파)
>   - `오차를 거꾸로 전파(backpropagate)`하면서 **가중치를 업데이트**하는 알고리즘

### Activation Functions (활성화 함수)
- `활성화 함수`는 신경망에서 **뉴런의 출력에 비선형성을 부여**
  - **이를 통해 비선형적인 관계를 학습**

| 함수          | 출력 범위   | 중심   | 장점              | 단점               |
| ----------- | ------- | ---- | --------------- | ---------------- |
| **Sigmoid** | 0 \~ 1  | 0 아님 | 부드럽고 확률 해석 가능   | 기울기 소실, 0 중심 아님  |
| **tanh**    | -1 \~ 1 | 0    | 0 중심, 더 강한 비선형성 | 기울기 소실 (깊은 네트워크) |

#### 🔹 1. 시그모이드 함수 (Sigmoid Function)
- ![img.png](images/ch08/img_1.png)
- ![img.png](images/ch08/img_2.png)
- 📈 특징
  - 출력 범위: 0 ~ 1
  - step function의 부드러운 근사 
  - `이진 분류(binary classification)` or 어떤 함수나 데이터의 값을 부드러운 형태로 근사시키는 것(smooth approximation) 에 적합
- ✅ 장점 
  - 시그모이드 함수는 곡선이 연속적이고 부드러워 **미분 가능** 
    - **경사 하강법(gradient descent)에 유리** 
  - 출력이 확률처럼 해석 가능 (0~1 범위)
- ❌ 단점 
  - `기울기 소실(Vanishing Gradient) 문제`
    - 입력이 크거나 작을 경우 시그모이드의 출력은 0 또는 1 근처로 포화 
    - 해당 데이터를 미분시 `기울기가 0`에 가까워져 학습이 제대로 이루어지지 않는 문제가 발생
  - 출력이 0 중심이 아님 → 가중치 최적화 시 비대칭적인 업데이트가 발생(한쪽으로 치우쳐짐)

#### 🔹 2. 하이퍼볼릭 탄젠트 함수 (tanh)
- ![img.png](images/ch08/img_3.png)
- ![img.png](images/ch08/img_4.png)
- 📈 특징 
  - 출력 범위: -1 ~ 1 
  - 시그모이드 함수와 유사하지만, **0 중심(zero-centered)**
- ✅ 장점 
  - 출력이 0을 중심으로 대칭 → 가중치 업데이트 시 안정적 
  - **비선형성이 강해 복잡한 데이터 패턴을 더 잘 표현 가능**
- ❌ 단점 
  - `기울기 소실(Vanishing Gradient) 문제`
    - 입력값이 극단적으로 크거나 작을 경우 기울기가 0에 가까워짐

> 현대 딥러닝에서는 **ReLU**나 **Leaky ReLU** 같은 다른 함수들을 더 많이 사용

#### 🔹 3. ReLU 활성화 함수 (Rectified Linear Unit)
- ![img.png](images/ch08/img_5.png)
  - 입력값이 양수이면 그대로 통과 
  - 음수이면 0으로 치환됩니다.
- ![img.png](images/ch08/img_6.png)
- `ReLU(Rectified Linear Unit)` 함수는 아주 단순하지만, **딥러닝에서 가장 널리 쓰이는 활성화 함수**
- ✅ 장점
  - 간단한 구현과 빠른 연산 속도
    - 복잡한 연산 없이, `단순히 0과 입력값 중 큰 값을 선택하는 구조`라서 계산이 빠르고 효율적
    - 모델 학습 속도가 빠름
  - 기울기 소실(Vanishing Gradient) **문제 완화**
    - `ReLU의 기울기`는 입력이 양수일 때 항상 1, 깊은 네트워크에서도 학습이 잘 진행
- ❌ 단점 
  - 음수 입력에 대해 0 출력 → `정보 손실 가능성`
    - 뉴런이 완전히 죽어버릴(dead neuron)
  - 0에서 미분 불연속 → 최적화 이슈 가능성
    - 특정 상황에서는 **학습 불안정이 발생**

#### 4 3. Leaky ReLU 활성화 함수 (Rectified Linear Unit)
- `Leaky ReLU`는 ReLU 함수의 확장 버전
  - **입력값이 음수**일 때도 `아주 작은 기울기를 가지도록 설계`된 함수 
- ![img.png](images/ch08/img_7.png)
- ![img.png](images/ch08/img_8.png) 
- ✅ 장점
  - 죽은 뉴런 문제 해결
    - Leaky ReLU는 음수 입력도 아주 작게 출력하므로, 뉴런이 죽지 않고 `미세하게라도 학습에 참여`
  - 전체 입력 구간에서 **기울기가 존재**
    - ReLU는 x=0에서 기울기가 끊기지만, Leaky ReLU는 음수 구간에도 기울기가 있어서 미분 가능한 연속 함수
- ❌ 단점
  - 음수 구간의 기울기(0.01 등)는 하이퍼파라미터 → 튜닝 필요
    - 이 값을 너무 작게 하면 ReLU와 차이 없음
    - 너무 크게 하면 음수 쪽으로 과도하게 학습

### Backpropagation (역전파)
- Backpropagation
  - 신경망을 학습시키기 위해 사용하는 핵심 알고리즘
  - Backward Propagation of Errors의 줄임말
  - **오차를 뒤로 전파하여 가중치를 조정하는 방식**

#### 🧠 전체 학습 과정 요약

### 1. **가중치와 편향 초기화**
- 신경망의 가중치(Weights)와 편향(Biases)을 무작위로 초기화
  - 학습이 반복되면서 점점 조정

### 2. **순전파 (Forward Propagation)**
- 각 뉴런:
  - **가중치 × 입력값 + 편향 → 활성화 함수 적용 → 다음 층으로 전달**

### 3. **손실 계산 (Loss Function)**
- 예측값과 실제값(정답)을 비교하여 **오차**(Loss)를 계산
  - 예: 회귀 문제에서는 MSE (Mean Squared Error) 사용

### 4. **역전파 (Backpropagation)**
- **출력층부터 입력층 방향으로 오차를 역으로 전달하며 기울기(gradient)를 계산**
- **체인 룰(Chain Rule)을 이용** - 체인 룰 : 여러 함수가 겹쳐져서 연결(composition) 되어 있을 때, 그 전체 함수의 미분값(기울기)을 계산하는 방법
  - 각 뉴런의 출력값에 대해 `손실이 얼마나 민감한지 계산`
    - 이를 바탕으로 가중치와 편향의 기울기(변화량)를 구함

### 5. **가중치와 편향 업데이트**
- 기울기 정보를 기반으로 가중치와 편향을 업데이트
- 편향도 마찬가지로 `학습률(learning rate)` 만큼 이동
- 이 때, 데이터를 여러 개씩 묶어 **batch 단위**로 처리

### 6. **에포크 반복 (Epochs)**
- 위의 과정을 **전체 데이터셋에 대해 여러 번 반복**
  - 1 Epoch = 전체 데이터를 한 번 다 학습한 것
- 일반적으로 수십~수천 번 반복하며 학습이 수렴되도록 함

### 7. **검증 (Validation / Test)**
- 학습이 끝나면, **보지 않았던 검증 데이터**로 성능을 측정
- 모델이 **과적합 없이 일반화**되었는지 평가

> NOTE
> - Learning Rate (학습률)
>   - 신경망의 가중치가 얼마나 빠르게 또는 천천히 업데이트될지를 결정하는 값
>     - 학습률이 크면: 더 빠르게 학습하지만 불안정할 수 있음
>     - 학습률이 작으면: 안정적으로 학습되지만 학습 속도 느림
> - Batch Size (배치 크기)
>   - 한 번의 학습 단계에서 몇 개의 데이터 샘플을 처리할지를 정하는 값
>     - 너무 작으면: 노이즈가 심하고 불안정 
>     - 너무 크면: 메모리 부담이 크고 학습 속도 저하

| 크기                        | 특징                                                                                         |
| ------------------------- | ------------------------------------------------------------------------------------------ |
| **작은 배치** (16, 32, 64)    | 자주 가중치를 업데이트함 → 빠르게 수렴할 수도 있음<br>다양한 데이터 반영 가능<br>하지만 기울기 계산에 **노이즈** 많음 → 수렴이 불안정할 수도 있음  |
| **큰 배치** (128, 256, 그 이상) | 더 **안정적인 기울기 계산**<br>병렬처리에 유리 → GPU 등에서 빠르게 학습 가능<br>하지만 메모리 많이 필요, 업데이트 빈도 낮아 수렴이 느릴 수 있음 |
