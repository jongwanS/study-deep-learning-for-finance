# Chapter 8. Deep Learning for Time Series Prediction I
- `딥러닝(Deep Learning)`은 머신러닝보다 약간 더 복잡하고 세부적인 분야
  - 머신러닝과 딥러닝은 모두 **데이터 과학**(Data Science)에 속한다.
- `딥러닝`은 대부분 **신경망(Neural Networks)에 관한 것**
  - `신경망`은 **매우 정교하고 강력한 알고리즘**
  - `신경망`은 **복잡하고 비선형적인 변수 간 관계도 잘 포착**할 수 있는 **강력한 도구**
## A Walk Through Neural Networks
- `인공신경망`(**ANN**: Artificial Neural Network)은 `신경과학`(Neurology) 연구에서 시작
  - `신경과학` 에서 인간의 뇌와 복잡하게 연결된 뉴런 네트워크가 어떻게 작동하는지를 이해하고자 했음
  - 이를 바탕으로 생물학적 신경망을 `컴퓨터적으로 모방`한 모델이 **인공신경망**

#### 🧠 인공신경망의 구조
- `인공신경망(ANN)`은 **노드**(=인공 뉴런)들이 **계층적으로 연결된 구조**
- 입력층 → 은닉층(1개 이상) → 출력층
  1. **입력층 (Input Layer)**
  - `데이터가 처음 입력되는 지점`
    - 수치형, 범주형, 센서 데이터 등 다양한 입력 가능

  2. **은닉층 (Hidden Layers)**
  - `입력 데이터를 처리`하는 계층 (1개 이상 가능)
  - 각 뉴런은 입력값을 받아 계산을 수행하고, `결과를 다음 계층에 전달`

  3. **출력층 (Output Layer)**
  - `마지막 예측 결과를 생성`
    - 문제 유형(분류/회귀 등)에 따라 뉴런의 개수와 출력 형태가 달라짐

- ![img.png](images/ch08/img.png)
- **인공신경망**(ANN: Artificial Neural Network)의 간단한 예시
  - 2개의 입력 노드에서 시작, **4개의 은닉층**(hidden layers)을 거쳐 `계산`이 이루어짐
  - 최종적으로 **출력층**(output layer)에서 **가중 예측**(weighted prediction)을 출력

#### 🧠 각 뉴런(Neuron)의 주요 역할
- **가중 입력 계산**
  - `이전 층 또는 입력 데이터`로 부터 **입력값**(input)을 받음
  - 각 입력값은 **가중치(weight)와 곱해짐**
    - `가중치`는 해당 입력의 **중요도 또는 연결 강도를 의미**
    - 모든 가중 입력을 **합산**(sum)함
      - 은닉층의 각 뉴런이 이전 층의 모든 뉴런으로부터 입력을 받아서 그것들을 하나로 모아 계산
- **활성화 함수 적용 (Activation Function)**
  - 가중 합을 기반으로 비선형 함수를 적용함 
  - 이 과정을 통해 뉴런의 **출력값**(output)이 결정됨 
  - `활성화 함수`는 모델이 **비선형 패턴을 학습할 수 있게 해줌**

#### 🔁 학습 과정 (Training)
- `신경망`은 학습 중에 **가중치를 조정함으로써 성능을 향상**
  - 일반적으로 `경사하강법(gradient descent)` 같은 최적화 알고리즘을 사용
- **손실 함수**(loss function)를 통해 `모델의 예측 오류를 계산`
  - 손실 함수의 **기울기**(gradient)를 계산하여, **가중치를 오차를 줄이는 방향으로 업데이트**

> Note
> - Forward Propagation (순전파) 
>   - 입력 데이터를 입력층에 넣고 
>   - 각 은닉층을 거치며 가중치 × 입력값 + 편향 → 활성화 함수를 통해 계산
>   - 예측 결과를 출력하는 전체 흐름
> - Backpropagation (역전파)
>   - `오차를 거꾸로 전파(backpropagate)`하면서 **가중치를 업데이트**하는 알고리즘

### Activation Functions (활성화 함수)
- `활성화 함수`는 신경망에서 **뉴런의 출력에 비선형성을 부여**
  - **이를 통해 비선형적인 관계를 학습**

| 함수          | 출력 범위   | 중심   | 장점              | 단점               |
| ----------- | ------- | ---- | --------------- | ---------------- |
| **Sigmoid** | 0 \~ 1  | 0 아님 | 부드럽고 확률 해석 가능   | 기울기 소실, 0 중심 아님  |
| **tanh**    | -1 \~ 1 | 0    | 0 중심, 더 강한 비선형성 | 기울기 소실 (깊은 네트워크) |

#### 🔹 1. 시그모이드 함수 (Sigmoid Function)
- ![img.png](images/ch08/img_1.png)
- ![img.png](images/ch08/img_2.png)
- 📈 특징
  - 출력 범위: 0 ~ 1
  - step function의 부드러운 근사 
  - `이진 분류(binary classification)` or 어떤 함수나 데이터의 값을 부드러운 형태로 근사시키는 것(smooth approximation) 에 적합
- ✅ 장점 
  - 시그모이드 함수는 곡선이 연속적이고 부드러워 **미분 가능** 
    - **경사 하강법(gradient descent)에 유리** 
  - 출력이 확률처럼 해석 가능 (0~1 범위)
- ❌ 단점 
  - `기울기 소실(Vanishing Gradient) 문제`
    - 입력이 크거나 작을 경우 시그모이드의 출력은 0 또는 1 근처로 포화 
    - 해당 데이터를 미분시 `기울기가 0`에 가까워져 학습이 제대로 이루어지지 않는 문제가 발생
  - 출력이 0 중심이 아님 → 가중치 최적화 시 비대칭적인 업데이트가 발생(한쪽으로 치우쳐짐)

#### 🔹 2. 하이퍼볼릭 탄젠트 함수 (tanh)
- ![img.png](images/ch08/img_3.png)
- ![img.png](images/ch08/img_4.png)
- 📈 특징 
  - 출력 범위: -1 ~ 1 
  - 시그모이드 함수와 유사하지만, **0 중심(zero-centered)**
- ✅ 장점 
  - 출력이 0을 중심으로 대칭 → 가중치 업데이트 시 안정적 
  - **비선형성이 강해 복잡한 데이터 패턴을 더 잘 표현 가능**
- ❌ 단점 
  - `기울기 소실(Vanishing Gradient) 문제`
    - 입력값이 극단적으로 크거나 작을 경우 기울기가 0에 가까워짐

> 현대 딥러닝에서는 **ReLU**나 **Leaky ReLU** 같은 다른 함수들을 더 많이 사용

#### 🔹 3. ReLU 활성화 함수 (Rectified Linear Unit)
- ![img.png](images/ch08/img_5.png)
  - 입력값이 양수이면 그대로 통과 
  - 음수이면 0으로 치환됩니다.
- ![img.png](images/ch08/img_6.png)
- `ReLU(Rectified Linear Unit)` 함수는 아주 단순하지만, **딥러닝에서 가장 널리 쓰이는 활성화 함수**
- ✅ 장점
  - 간단한 구현과 빠른 연산 속도
    - 복잡한 연산 없이, `단순히 0과 입력값 중 큰 값을 선택하는 구조`라서 계산이 빠르고 효율적
    - 모델 학습 속도가 빠름
  - 기울기 소실(Vanishing Gradient) **문제 완화**
    - `ReLU의 기울기`는 입력이 양수일 때 항상 1, 깊은 네트워크에서도 학습이 잘 진행
- ❌ 단점 
  - 음수 입력에 대해 0 출력 → `정보 손실 가능성`
    - 뉴런이 완전히 죽어버릴(dead neuron)
  - 0에서 미분 불연속 → 최적화 이슈 가능성
    - 특정 상황에서는 **학습 불안정이 발생**

#### 4 3. Leaky ReLU 활성화 함수 (Rectified Linear Unit)
- `Leaky ReLU`는 ReLU 함수의 확장 버전
  - **입력값이 음수**일 때도 `아주 작은 기울기를 가지도록 설계`된 함수 
- ![img.png](images/ch08/img_7.png)
- ![img.png](images/ch08/img_8.png) 
- ✅ 장점
  - 죽은 뉴런 문제 해결
    - Leaky ReLU는 음수 입력도 아주 작게 출력하므로, 뉴런이 죽지 않고 `미세하게라도 학습에 참여`
  - 전체 입력 구간에서 **기울기가 존재**
    - ReLU는 x=0에서 기울기가 끊기지만, Leaky ReLU는 음수 구간에도 기울기가 있어서 미분 가능한 연속 함수
- ❌ 단점
  - 음수 구간의 기울기(0.01 등)는 하이퍼파라미터 → 튜닝 필요
    - 이 값을 너무 작게 하면 ReLU와 차이 없음
    - 너무 크게 하면 음수 쪽으로 과도하게 학습

### Backpropagation (역전파)
- Backpropagation
  - 신경망을 학습시키기 위해 사용하는 핵심 알고리즘
  - Backward Propagation of Errors의 줄임말
  - **오차를 뒤로 전파하여 가중치를 조정하는 방식**

#### 🧠 전체 학습 과정 요약

1. **가중치와 편향 초기화**
- 신경망의 가중치(Weights)와 편향(Biases)을 무작위로 초기화
  - 학습이 반복되면서 점점 조정

2. **순전파 (Forward Propagation)**
- 각 뉴런:
  - **가중치 × 입력값 + 편향 → 활성화 함수 적용 → 다음 층으로 전달**

3. **손실 계산 (Loss Function)**
- 예측값과 실제값(정답)을 비교하여 **오차**(Loss)를 계산
  - 예: 회귀 문제에서는 MSE (Mean Squared Error) 사용

4. **역전파 (Backpropagation)**
- **출력층부터 입력층 방향으로 오차를 역으로 전달하며 기울기(gradient)를 계산**
- **체인 룰(Chain Rule)을 이용** - 체인 룰 : 여러 함수가 겹쳐져서 연결(composition) 되어 있을 때, 그 전체 함수의 미분값(기울기)을 계산하는 방법
  - 각 뉴런의 출력값에 대해 `손실이 얼마나 민감한지 계산`
    - 이를 바탕으로 가중치와 편향의 기울기(변화량)를 구함

5. **가중치와 편향 업데이트**
- 기울기 정보를 기반으로 가중치와 편향을 업데이트
- 편향도 마찬가지로 `학습률(learning rate)` 만큼 이동
- 이 때, 데이터를 여러 개씩 묶어 **batch 단위**로 처리

6. **에포크 반복 (Epochs)**
- 위의 과정을 **전체 데이터셋에 대해 여러 번 반복**
  - 1 Epoch = 전체 데이터를 한 번 다 학습한 것
- 일반적으로 수십~수천 번 반복하며 학습이 수렴되도록 함

7. **검증 (Validation / Test)**
- 학습이 끝나면, **보지 않았던 검증 데이터**로 성능을 측정
- 모델이 **과적합 없이 일반화**되었는지 평가

> NOTE
> - Learning Rate (학습률)
>   - 신경망의 가중치가 얼마나 빠르게 또는 천천히 업데이트될지를 결정하는 값
>     - 학습률이 크면: 더 빠르게 학습하지만 불안정할 수 있음
>     - 학습률이 작으면: 안정적으로 학습되지만 학습 속도 느림
> - Batch Size (배치 크기)
>   - 한 번의 학습 단계에서 몇 개의 데이터 샘플을 처리할지를 정하는 값
>     - 너무 작으면: 노이즈가 심하고 불안정 
>     - 너무 크면: 메모리 부담이 크고 학습 속도 저하

| 크기                        | 특징                                                                                         |
| ------------------------- | ------------------------------------------------------------------------------------------ |
| **작은 배치** (16, 32, 64)    | 자주 가중치를 업데이트함 → 빠르게 수렴할 수도 있음<br>다양한 데이터 반영 가능<br>하지만 기울기 계산에 **노이즈** 많음 → 수렴이 불안정할 수도 있음  |
| **큰 배치** (128, 256, 그 이상) | 더 **안정적인 기울기 계산**<br>병렬처리에 유리 → GPU 등에서 빠르게 학습 가능<br>하지만 메모리 많이 필요, 업데이트 빈도 낮아 수렴이 느릴 수 있음 |

#### Optimization Algorithms
- `신경망(Neural Networks)`에서는 최적화 알고리즘을 사용하여 학습 과정 중 **네트워크의 파라미터(가중치와 편향)를 업데이트**
- 최적화 알고리즘을 통해 **손실 함수(loss function)를 최소화**하고, 네트워크의 성능을 최대로 만드는 최적의 파라미터 값을 찾는 것이 목적

**[최적화 알고리즘(Optimizers)]**  
**1. 경사 하강법(Gradient Descent, GD)**
  - `기본적인 최적화 알고리즘`
  - 손실 함수의 **기울기**(gradient)에 대해 **반대 방향으로 가중치와 편향을 업데이트** => 최소 또는 최대값을 찾음.
  - **학습률(learning rate)을 곱한 만큼 이동**하면서 `파라미터를 조정`
    - ![img.png](images/ch08/img_9.png)

**2. 확률적 경사 하강법(Stochastic Gradient Descent, SGD)**
  - SGD는 `경사 하강법의 변형`, 전체 데이터셋이 아니라 **임의로 선택한 하나의 훈련 샘플 또는 미니배치 (mini batch)를** 사용해 `기울기를 계산하고 파라미터를 업데이트` 한다.
  - 계산 효율성이 높고, 훈련 과정에서 노이즈가 발생하여 **지역 최적점(local optima)에서** 벗어나는 데 도움을 줄 수 있다.
    - ![img.png](images/ch08/img_10.png)
    - ![img.png](images/ch08/img_11.png)

**3. Adam (Adaptive Moment Estimation)** - Adam = RMSprop + Momentum
  - `Adam은 적응형 최적화 알고리즘`, 각 파라미터마다 **1차 및 2차 모멘트(평균과 분산에 해당)를 추정**하여 `적응형 학습률을 계산`한다. **=> 학습률을 동적으로 계산**
    - **1차 모멘트**: 기울기(gradient)의 평균
      - `파라미터가 어느 방향으로 가야 할까?`를 파악
    - **2차 모멘트**: 기울기의 분산 또는 제곱 평균
      - `얼마나 불안정하게 튀고 있는가?`를 측정 
  - Adam은 `다양한 상황에서 효율성과 성능이 뛰어나` 널리 사용된다.
  - **Adam =** `방향은 부드럽게 잡고 (Momentum)` + `불안정한 곳은 속도를 줄여서 이동 (RMSprop)` **=> 학습률을 매 순간 파라미터마다 다르게 조정**
  - 각 파라미터마다 **학습률을 다르게 적용** -> `가중치마다 얼마나 빠르게 움직일지를 조절`
  - 최근의 **그래디언트 정보를 누적해서 활용** -> `이전보다 지금 어디로 움직이면 좋을지 계산함`.



**4. RMSprop (Root Mean Square Propagation)**
  - RMSprop은 기본적인 `경사 하강법`의 **느린 수렴이나 진동 문제를 해결하기 위해 개발된 알고리즘**이다.
  - 최근의 **제곱 기울기(squared gradients)의 평균을 계산**하여 `각 파라미터에 대해 학습률을 조정`한다. **=> 학습률을 동적으로 계산**
  - **지수 가중 이동 평균(exponentially weighted moving average)을 사용**하여 `기울기의 크기를 조절`한다.
  - 변동이 큰 파라미터에는 학습률을 낮추고, 변동이 적은 곳은 높여서 조정
  - 각 파라미터마다 **학습률을 다르게 적용** -> `가중치마다 얼마나 빠르게 움직일지를 조절`
  - 최근의 **그래디언트 정보를 누적해서 활용** -> `이전보다 지금 어디로 움직이면 좋을지 계산함`.
- [그림 예시]
  ![img.png](images/ch08/img_12.png)  
  - 곡선 (회색): 손실 함수 (Loss function), 우리가 줄이고 싶은 대상
  - 검은 점들: 현재 파라미터의 위치
  - 보라색 화살표: 파라미터가 업데이트된 방향과 거리 → 한 스텝당 이동량 
  - **어떤 화살표는 길고 어떤 화살표는 짧음 ➡ 이게 바로 RMSprop의 핵심: 상황에 따라 학습률을 조절해서 이동 크기를 다르게 한다**


> **최적의 옵티마이저를 선택하기 위해 지속적인 실험 & 튜닝은 필수이다.**


#### Regularization Techniques(정규화 기법)
- 신경망에서 `정규화 기법`은 **과적합을 방지하는 데 사용**되는 방법 
- `정규화 기법`은 **신경망의 복잡성을 제어**하고 보지 못한 **데이터에 대한 일반화 능력을 향상**시키는 데 도움이 된다.

---
  
- **드롭아웃(Dropout)**
  - `드롭아웃`은 **과적합을 방지하기 위해 신경망에서 흔히 사용되는 정규화 기법**
    - **신경망이 특정 부분에 너무 의존해서 생기는 문제를 막는 방법**
  - 이 기법은 `훈련 중에 특정 비율의 뉴런을 무작위로 생략(드롭)`하여 출력을 0으로 설정하는 방식으로 작동
    - **특정 비율을 뉴런을 훈련에서 배제**
    - 배제한 뉴런들러 인해 **나머지 뉴런들이 더 견고하고 독립적인 표현을 학습하도록 강제**
  - **각 뉴런의 개별 학습 능력을 향상**

- **조기 종료(Early Stopping)**
  - `과적합 방지 기법 중 하나`
  - `검증 세트`에 대한 **성능이 일정 기간 동안 더 이상 개선되지 않거나 오히려 악화되기 시작하는 시점을 포착**
    - 이 시점은 **모델이 훈련 데이터의 노이즈까지 학습**하여 `일반화 능력이 떨어지기 시작하는 과적합의 징후로 간주`
  - **훈련을 에포크(Epoch) 단위로 진행**하면서 `매 에포크마다 검증 세트 성능을 평가`
    - 검증 성능이 가장 좋았던 시점의 모델 상태를 기록 -> `검증 성능이 가장 좋았을 때 기록해 둔 모델을 사용`


#### Multilayer Perceptrons, MLP (다층 퍼셉트론)
- `퍼셉트론의 구성 요소 및 작동 방식`
  - ![img.png](images/ch08/img_13.png)
    - **입력 (Inputs)** : 퍼셉트론이 받아들이는 여러 개의 정보 또는 데이터
    - **가중치 (Weights)** : 각 입력값에 곱해지는 중요도 또는 영향력을 나타내는 값
    - **가중 합 (Weighted Sum)** : 각 입력값(x)과 해당 가중치(w)를 곱한 값들을 모두 더한 것
    - **활성화 함수 (Activation Function)** : 가중 합의 결과를 받아 최종 출력을 결정하는 함수
    - **출력 (Output)** : 활성화 함수를 거쳐 나온 퍼셉트론의 최종 결과값
- `퍼셉트론`
  - **단일 계층 신경망**
  - **가장 기본적인 형태의 의사결정 단위**

- `다층 퍼셉트론(MLP)`
  - 여러 계층의 인공 뉴런, 즉 **노드로 구성된 인공 신경망(ANN)의 한 종류**
  - `MLP`는 **피드포워드 신경망** - 정보가 네트워크를 통해 `한 방향으로만 흐른다는 것을 의미`
  - `MLP`는 **역전파(backpropagation) 알고리즘을 사용**하여 훈련
    - `역전파`는 예측된 출력과 원하는 출력 사이의 차이(오차)를 최소화하기 위해 네트워크 내 **뉴런들의 가중치(weights)를 조정**
  - `MLP`는 **비선형적인 관계를 학습하는 능력이 뛰어남**
  - ![img.png](images/ch08/img_14.png)
    - `1-2-3-4 한번의 반복`이 **1 epoch** 이다. => **오차가 줄어들때까지 진행**

````python
'''
은닉층 1: 20개 뉴런, ReLU
은닉층 2: 20개 뉴런, ReLU
출력층: 1개 뉴런 (다음 시점 수익률 예측)
'''
model = Sequential()

# 은닉층1
model.add(Dense(num_neurons_in_hidden_layers, input_dim = num_lags, activation = 'relu'))  
# 은닉층2
model.add(Dense(num_neurons_in_hidden_layers, activation = 'relu')) 

#출력층 1개
model.add(Dense(1))
'''
손실 함수: 평균 제곱 오차(MSE)
최적화 기법: Adam
'''
model.compile(loss = 'mean_squared_error', optimizer = 'adam')
````
- 모델 결과
  - ![img.png](images/ch08/img_15.png)
- 하이퍼파라미터가 민감하게 결과에 영향을 준다.
- 특징 선택, 모델 구조, 정규화, 앙상블 등 다양한 방법을 통해 모델 성능을 개선해야 한다.
- 훈련 중에는 손실 값이 점점 감소해야 학습이 잘 되고 있다는 신호이다.
  - ![img.png](images/ch08/img_16.png)

#### Recurrent Neural Networks

#### Long Short-Term Memory

#### Temporal Convolutional Neural Networks