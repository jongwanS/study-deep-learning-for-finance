# Chapter 7. Machine Learning Models for Time Series Prediction (시계열 예측을 위한 머신러닝 모델)
- `머신러닝`
  - 인공지능(AI)의 하위 분야
  - 컴퓨터가 명시적으로 프로그래밍되지 않아도 `학습`하고 `예측`이나 **결정**을 내릴 수 있도록 하는 **알고리즘과 모델의 개발**에 초점을 맞춘다. ==> 따라서, 그래서 `학습`이라는 용어가 사용
  - 일반적으로 `대량의 데이터를 분석`하고 그 안에서 **패턴을 추출**함으로써 이루어진다.
## The Framework
- 프레임워크는 연구 전반의 과정을 조직화하는 중요한 요소(데이터 수집부터 성능 평가까지)
- 적절한 프레임워크를 갖추는 것이 중요한 이유 
  - 백테스트 전반에 **일관성을 보장**
  -  다양한 머신러닝 모델 간의 **비교**가 가능
- 프레임워크는 일반적으로 `다음과 같은 순서`로 진행
  1. **데이터 가져오기 및 전처리**
     - 예측 모델을 제대로 백테스트하고 평가하려면 충분한 양의 데이터를 가져와야 한다. 
  2. **학습-테스트 분할 수행**
     - 이과정은 데이터를 두 부분으로 나누는 것
       - 첫 번째 부분은 알고리즘을 학습시키는 데 사용(미래 값을 예측하기 위한 수학적 공식 추출)
       - 두 번째 부분은 알고리즘이 한 번도 본 적 없는 데이터에 대해 얼마나 잘 작동하는지를 테스트
  3. **알고리즘을 사용해 학습(피팅)하고 예측(테스트) 수행**
  4. **성능 평가 알고리즘 실행**
     - 모델이 과거 데이터를 얼마나 잘 예측했는지 이해하기 위해 평가를 수행

> **Note**  
> - 학습용 데이터셋은 **인샘플(in-sample)** 데이터 라고도 한다.
> - 테스트용 데이터셋은 **아웃샘플(out-of-sample)** 데이터 라고도 한다.

- `학습용(in-sample)` 데이터셋에서는 모델을 학습시켜 **예측 함수**(forecasting function)를 도출
- `테스트용(out-of-sample)` 데이터셋에서는 학습 데이터로 계산된 예측 함수를 적용해 그 **성능을 평가**

#### master_function.py
- 시계열 데이터를 전처리(변환), 네 개의 배열(또는 원한다면 데이터프레임)로 나눈다.
- **데이터를 나누기 전에 중요한 점**
  - 무엇을 예측할 것인지(종속 변수)
  - 무엇을 가지고 예측할 것인지(독립 변수)를 명확히 아는 것

- `배열 x_train` (모델이 공부하는 데이터)
  - 예측하고자 하는 변수의 변동을 설명해주는 **인샘플 독립 변수 집합**
  - **예측을 위한 입력값 또는 설명 변수(predictors)**
- `배열 y_train` (모델이 공부하는 데이터의 해답지)
  - 모델이 예측 함수를 학습할 수 있도록 돕는, **인샘플 종속 변수 집합**
  - **정답 데이터**
- `배열 x_test` (모델이 시험 보는 데이터)
  - 모델의 성능을 평가하기 위해 사용하는 **아웃샘플 독립 변수 집합**
  - **모델이 한 번도 본 적 없는 데이터**에 대한 테스트 입력값
- `배열 y_test` (시험의 해답지)
  - 모델이 예측해야 하는 실제 값들
  - 예측 결과와 비교할 **정답 값**

````text
master_function.py
> 최근 500개의 일별 EUR/USD 수익률이 현재 수익률에 대한 예측력
````
- 종속 변수 (예측값)
  - EUR/USD의 t+1 시점의 일간 수익률 (y 변수)
- 독립 변수 (입력값)
  - EUR/USD의 이전 500일간의 일간 수익률 (x 변수)

#### 더미 회귀 모델(dummy regression model)
- `더미 회귀`
  - 아주 간단한 규칙만을 사용하여 예측하는 비교용 머신러닝 알고리즘
  - 실제 예측 가치가 거의 없기 때문에 벤치마크(기준선)로만 사용
  - 더미 회귀의 `유용성`은, **만든 모델**이 이 모델보다 뛰어난지 여부를 확인하는 데 있다.
- 더미 회귀 과정
  1. 데이터를 가져온다.
  2. 데이터를 전처리하고 분할한다.
  3. 알고리즘을 학습시킨다. 
  4. 학습한 파라미터를 이용해 테스트 데이터에 대해 예측을 수행한다.  
     또한, 비교를 위해 학습 데이터에 대해서도 예측을 수행한다.
  5. 결과를 그래프로 나타내고 평가한다. 

- ![img.png](images/img_30.png)


- 더미 회귀(dummy regression)는 다음 전략 중 하나를 인자로 받을 수 있다
  - `mean (평균)`
    - 항상 학습 데이터(y_train)의 평균을 예측
  - `median (중앙값)`
    - 항상 학습 데이터의 중앙값을 예측
  - `quantile (분위수)`
    - 분위수 매개변수와 함께 제공될 경우, 학습 데이터의 지정된 분위수를 예측
  - `constant (상수)`
    - 사용자가 제공한 일정한 값을 항상 예측

- 앞서 본 코드에서 선택된 전략은 **mean**
  - 더미 회귀 전략을 사용할 경우 모든 예측 값은 단순히 학습 데이터(y_train)의 평균이 됩니다.
  - 더미 회귀 한계
    - 머신러닝 모델이 아니라 **기준선**(benchmark)으로만 사용된다.(더미 회귀는 학습하지 않고 평균 같은 단순한 값만 예측하기 때문)


- 모델이 잘 작동하는지 어떻게 알 수 있을까
  - `성능 평가`는 트레이딩과 알고리즘 개발에서 매우 중요한 개념
  - `성능 평가`를 통해 올바른 모델을 선택하고 실제 환경에 적용
- **성능 평가** 방법 크게 두 가지
  - `모델 평가` (Model Evaluation)
    - 알고리즘이 **예측을 얼마나 잘하는지 평가**
  - `트레이딩 평가`(Trading Evaluation)
    - 그 알고리즘을 사용해서 실제 매매했을 때 **금융 성과를 평가**

#### 모델 평가에서 자주 쓰는 지표
- **정확도(Accuracy)**
  - 공식 : 정확도(Accuracy) = (정확하게 맞춘 예측 수) ÷ (전체 예측 수) × 100
    - 예를 들어, 작년에 100번의 예측을 했고 그중 73번이 맞았다면, 정확도는 73%
  - `상승` 또는 `하락` **방향을 예측**
  - 금융권에서는 정확도를 **히트 비율(hit ratio)** 이라고도 한다.
- **예측의 정확도**는 예측값(y_predicted)이 실제값(y_test)과 얼마나 가까운지로도 평가 ==> **손실 함수(loss function) 사용**
- **손실 함수**(loss function)
  - 정의 : 예측값과 실제값 사이의 차이를 수학적으로 계산하는 함수
  1. **평균 절대 오차**(Mean Absolute Error, MAE)
     - `가장 기본적인 손실 함수` 
     - `MAE`는 **예측값과 실제값의 절대 차이들의 평균을 계산**
       - ![img.png](images/img_22.png)  
       - MAE는 예측값과 실제값 사이의 평균적인 거리(절대값 차이)를 나타내며, **MAE 값이 작을수록 모델의 예측이 더 정확하다는 의미**
  2. **평균 제곱 오차**(Mean Squared Error, MSE)
     - `MSE`는 **예측값과 실제값의 차이를 제곱하여 평균한 값** ==> 회귀 문제에서 자주 사용
     - MAE와 마찬가지로, **MSE가 낮을수록 모델이 더 정확하다는 의미** ==> but MSE는 오차를 제곱하기 때문에, **큰 오차에 더 민감**하게 반응한다.
     - 하지만, MSE는 제곱된 단위를 가지기 때문에 해석이 직관적이지 않을 수 있음. ==> 이를 해결 위해, RMSE(Root Mean Squared Error, 평균 제곱근 오차) 나옴
     - `RMSE`는 **MSE의 제곱근을 취함**으로써, 원래 목표 변수와 같은 단위로 되돌려 준다.
       - `RMSE`는 **예측값이 실제값으로부터 얼마나 떨어져 있는지를 측정** ==> 표준편차(standard deviation) 와 동등한 개념

> Note
> - 데이터에 극단적인 값이 포함되어 있거나, 오차의 절대적인 크기가 중요할 때는 **MAE를 사용하는 것이 적절**
> - 모델의 성능을 더 민감하게 개선하고 싶을 때 주로 **MSE를 사용하는 것이 적절**
>   - 모델의 정밀한 조정(tuning)을 원할 때는 MSE가 더 유용

- MAE, MSE, RMSE 같은 지표는 **낮을수록 좋다.**
  - MAE: 평균 절대 오차 (예측이 얼마나 벗어났는지 평균적으로)
  - MSE: 평균 제곱 오차 (큰 오차에 더 큰 패널티)
  - RMSE: MSE의 제곱근 (단위가 원래 값과 같아짐)
- `숫자가 낮을수록 예측이 정확`하다는 뜻, 하지만 이 수치 하나만으로 모델의 좋고 나쁨을 판단하기는 어렵다.
  - **비교 대상**(베이스라인)이 있어야 의미가 생긴다.
- 여러 모델을 비교하여, **수용 가능한 오차 기준**(threshold)을 정해서 그 기준을 넘는 모델은 탈락시킨다. 
- 모델의 **편향**(Bias)도 살펴봐야 한다
  - 예측 모델이 한 방향(예: 매수)만 많이 예측하는 경우, `편향된 모델`
  - ![img.png](images/img_23.png)
    - 어떤 모델이 올해 934개의 매수 포지션(long)과 899개의 매도 포지션(short)을 가졌다면, 모델 편향 지표는 1.038이며, 이는 허용 가능한 수준
    - 모델 편향이 0.0이라는 것은 매수 신호가 전혀 없었다는 뜻 (매도 없으면 → 0으로 나누기 → 편향 값 정의 불가)

- 수익성 지표
  - **순수익(Net Return)**: 수수료 등을 뺀 `최종 이익`을 의미
    - ![img.png](images/img_24.png)
  - **이익 계수(Profit Factor)**
    - Profit Factor = 총 이익 / 총 손실
      - 1.00보다 크면 → 수익성 있는 전략
      - 1.00보다 작으면 → 손실 나는 전략
  - **개별 거래(individual trades)**
    - 거래당 평균 수익(Average Gain per Trade): 과거 데이터 기반으로, `수익이 난 거래들의 평균 이익`
    - 거래당 평균 손실(Average Loss per Trade): 과거 데이터 기반으로, `손실이 난 거래들의 평균 손해`
    - ![img.png](images/img_25.png)
  - **위험(Risk)**
    - 최대 낙폭(Maximum Drawdown) : 투자 자산이나 포트폴리오의 가치가 최고점에서 최저점까지 얼마나 많이 하락했는지를 나타내는 지표
      - 이 지표는 자산이 `얼마나 크게 손실될 수 있는지를 보여주는 지표`
      - **하방 위험**(Downside Risk)을 평가할 때 자주 사용
      - ![img.png](images/img_26.png)
  - **샤프 비율(Sharpe Ratio)**
    - 위험을 얼마나 잘 감수하고 수익을 냈는지 보여주는 지표
      - ![img.png](images/img_27.png)
    - **샤프 비율 > 1.0**, 수익이 리스크보다 크다 = **좋은 전략**
    - **샤프 비율 < 1.0**, 리스크 대비 수익이 적다 = **비효율적 전략**
    - **샤프 비율 = 0**, 초과 수익 없음
    - **음수**, 무위험 수익률보다 못함 = **손실 전략**
#### 샤프비율 예시


  | 항목                  | 값                                             | 설명 |
| ------------------- |-----------------------------------------------| -- |
| **포트폴리오 수익률 (12%)** | 1년 동안 투자해서 총 **12% 수익**을 냈다는 뜻함.              |    |
| **무위험 수익률 (2%)**    | 아무런 위험 없이 얻을 수 있는 수익률. 예: **국채**나 **예금 이자율**. |    |
| **수익률 표준편차 (10%)**  | 이건 수익률의 변동성(위험도)를 뜻함. 숫자가 클수록 수익이 들쭉날쭉하다는 의미  |    |
- ![img.png](images/img_28.png)

> **리스크 프리 레이트 (Risk-Free Rate)**
> - 전혀 위험이 없는 투자에서 얻을 수 있는 이론적인 수익률
> - 다른 위험이 있는 투자들의 수익률을 평가할 때 **기준점(벤치마크) 역할**

- EUR-USD 모델 결과
  - ![img.png](images/img_29.png)
  - ![img.png](images/img_31.png)
  - **모델 편향(bias)이 0.0**이라는 것은, 이 모델이 `더미 회귀 모델(dummy regression model)`임을 뜻함
  - 여기서 편향(bias)이 0.0이라는 뜻은, 모델의 예측값이 **모두 하락(bearish) 방향임을 의미**

> **NOTE**
> - **적절한 백테스트**(backtest)를 위해서는 데이터를 반드시 **훈련 세트**(training set)와 테스트 **세트**(test set)로 나누어야 한다.
>   - 훈련 세트(training set)
>     - x_train: 예측에 사용될 독립 변수(입력값)
>     - y_train: 예측하려는 목표 변수(정답값)
>   - 테스트 세트(test set) : x_test로 예측을 수행, y_test 와 비교
>     - x_test: 모델이 한 번도 본 적 없는 입력값
>     - y_test: 그에 대한 실제 정답
> - 모델 학습(Fitting)
>   - 알고리즘이 **x_train**, **y_train**을 바탕으로 학습하는 과정
> - 모델 예측(Predicting)
>   - 학습한 모델이 **x_test**를 바탕으로 예측을 수행하는 과정
> - **알고리즘의 주요 목표**
>   - **높은 정확도(accuracy)** && **안정적이고 변동성이 낮은 수익률**을 달성

## Machine Learning Models
- 각 머신러닝 모델의 `강점`과 `약점`을 이해하는 것이 중요하다.
  - 예측 작업의 특성에 따라 **어떤 모델을 선택해야 할지 알 수 있기 때문**이다.

### Linear Regression (선형 회귀)
- **선형 회귀 알고리즘**
  - `목표`
    - **예측값과 실제값 사이의 제곱 오차의 합(Sum of Squared Differences)을 최소화하는 최적의 직선(line)을 찾는 것**
  - `최적화 기법`
    - 가장 많이 사용되는 방법은 **최소제곱법**(Ordinary Least Squares, OLS) 
    - 오차의 제곱합이 가장 작아지도록 **직선의 계수**(coefficient)를 조정
  - `학습 과정`
    - OLS를 이용해 학습 데이터로부터 독립 변수의 **최적 계수들을 계산**
    - 이 계수는 직선의 **기울기(slope)와 절편(intercept) 역할** ??
  - `최종 모델`
    - 예측값은 다음과 같은 선형 함수로 표현
    - ![img.png](images/img_32.png)
    - ![img.png](images/img_33.png)
      - 이 함수는 입력 변수들에 가중치를 곱한 값에 절편과 오차를 더해,**예측값**(예: 기대 수익)을 계산 ?? 

---
#### 1. 선형 회귀 도식
````yaml
[입력 데이터 X, Y]
   |
   v
[최소제곱법 (OLS)]
   → 예측값과 실제값의 오차²의 합을 최소화
   |
   v
[기울기 a, 절편 b 계산]
   |
   v
[선형 함수 생성]
   y = a * x + b + noise
   |
   v
[새로운 x 값으로 y 예측]
````
> 주어진 데이터에 가장 잘 맞는 직선을 그려서, 새로운 입력이 들어오면 해당 직선 위의 y값을 예측하는 구조
  
#### 2. 예시로 보는 선형 회귀
📊문제: 기온에 따라 아이스크림 판매량이 어떻게 변할까?

| 기온(℃) | 아이스크림 판매량(개) |
| ----- | ------------ |
| 20    | 100          |
| 22    | 120          |
| 25    | 150          |
| 27    | 170          |
| 30    | 200          |

🔍 선형 회귀의 역할
- 이런 데이터를 보고 선형 회귀는 다음과 같은 직선을 찾는다.
  - `판매량 = 5*기온+0`
    - 여기서 5는 기울기 → 기온이 1도 오르면, 판매량은 5개 증가
    - 0은 절편 → 단순화를 위해 여기선 0이라고 가정

❓ 예측해보기
- 기온이 28도일 때 아이스크림은 몇 개 팔릴까?
  - `판매량 = 5*28+0=140`
- 예측 결과: 약 140개 팔릴 것이라고 예측

> **선형 회귀**는 데이터의 흐름을 직선으로 잡아, **새로운 값이 들어왔을 때 결과를 예측**
---



✅ **제곱 오차의 합**이란?  
🎯 `목표`
- **직선이 데이터에 얼마나 잘 맞는지를 판단하는 기준**이 필요
- 그 기준이 바로 **오차의 제곱합**(Sum of Squared Errors)

📌 `오차(Error)`란?  
- 실제값 y가 있고, 우리가 만든 직선은 예측값(y`)
- 오차 = 실제값 - 예측값(즉, 점이 직선에서 얼마나 떨어져 있는지)

📌 `오차의 제곱합`이란?  
- 모든 데이터에 대해 오차를 각각 제곱해서 전부 더한 값
  - ![img.png](images/img_34.png)
- **이 값을 줄이는 게 선형 회귀의 목적** -> 이 값이 작을수록 직선이 데이터에 잘 맞는다는 뜻

✅ **최소제곱법**(OLS)이란?
- `오차의 제곱합이 가장 작아지는 기울기(a)와 절편(b)`를 **수학적으로 계산**하는 방법
  - 쉽게, 직선을 어떻게 그려야 오차가 가장 작을까? -> **수학적으로 푸는 방법이 바로 OLS**

🔍 아주 쉬운 예시
📊 데이터

| 공부시간(x) | 점수(y) |
| ------- | ----- |
| 1시간     | 50점   |
| 2시간     | 60점   |
| 3시간     | 70점   |

✅ 어떤 직선을 그릴까?
- y=a⋅x+b
  - 직선 후보 1 : y=10x+40
  - 직선 후보 2 : y=20x+30
- **OLS** 를 통해 가장 적합한 후보를 찾으면 된다.
---
✅ 예제) OLS 공식  

| 공부시간(x) | 점수(y) |
| ------- | ----- |
| 1시간     | 2점    |
| 2시간     | 4점    |
| 3시간     | 6점    |

- 1단계 : 먼저 평균을 구한다.
  - x 평균(𝑥̄`)= (1 + 2 + 3) ÷ 3 = 2
  - y 평균(y`) = (2 + 4 + 6) ÷ 3 = 4
- 2단계 : 기울기공식 
  - ![img.png](images/img_35.png)
  - (각 x가 평균에서 얼마나 떨어져있나) × (각 y가 평균에서 얼마나 떨어져있나) 를 다 더한 값 나누기
  - 각 x가 평균에서 얼마나 떨어져있나의 제곱을 다 더한 값
- 3단계: 계산

| i | $x_i$ | $y_i$ | $x_i - \bar{x}$ | $y_i - \bar{y}$ | 곱하기 $(x_i - \bar{x})(y_i - \bar{y})$ | 제곱 $(x_i - \bar{x})^2$ |
| - | ----- | ----- | --------------- | --------------- | ------------------------------------ | ---------------------- |
| 1 | 1     | 2     | 1 - 2 = -1      | 2 - 4 = -2      | (-1) × (-2) = 2                      | (-1)² = 1              |
| 2 | 2     | 4     | 2 - 2 = 0       | 4 - 4 = 0       | 0 × 0 = 0                            | 0² = 0                 |
| 3 | 3     | 6     | 3 - 2 = 1       | 6 - 4 = 2       | 1 × 2 = 2                            | 1² = 1                 |

- 4단계: 각 열을 모두 더하기
  - ![img.png](images/img_36.png)
- 5단계: 기울기 계산
  - `a = 4/2` **= 2**
- 6단계: 절편(b) 계산!
  - b = y`- a*x' = 4 -2*2 **= 0**

> **최종모델**
> - **y' = 2x+0**
---

✅ **선형 회귀 요약**
- `목표`: 데이터를 가장 잘 설명하는 **직선**(선형 함수)을 찾는 것
- `수식` : y' = ax + b
  - a: 기울기 (x가 1 늘 때 y가 얼마나 늘어나는지)
  - b: 절편 (x = 0일 때 y는 얼마인지)
- `손실 함수`
  - 예측값과 실제값 사이의 오차를 구하고, 그 **오차를 제곱해서 모두 더한 값**(제곱 오차의 합)을 최소화
- `최적화 기법`
  - **최소제곱법**(OLS)를 사용해, **기울기**(a)와 **절편**(b)을 수학적으로 계산


---
- ![img.png](images/ch07/img_37.png)
- ![img.png](images/ch07/img_38.png)
````text
Accuracy Train =  58.58 %
Accuracy Test =  49.64 %
→ 학습 데이터에선 58% 정도 예측이 맞았지만, 테스트 데이터에선 50%도 안 되는 정확도
→ 테스트셋 성능이 낮아, 일반화 능력이 부족함, 과거 데이터엔 어느 정도 맞지만, 미래(테스트셋)엔 거의 맞지 않음

RMSE Train =  0.0070994536 (평균 제곱근 오차)
RMSE Test =  0.0055892906
→ 예측값과 실제값의 차이(오차)의 평균 크기
→ 숫자가 작을수록 좋음
→ 절대 오차 자체는 크게 나쁘지 않음, 하지만 정확도와 상관관계를 함께 보면, 모델이 실제 방향성을 잘 못 잡고 있음


Correlation In-Sample Predicted/Train =  0.373
Correlation Out-of-Sample Predicted/Test =  0.014
→ 예측값과 실제값의 방향성 일치 정도 (1이면 완벽하게 같은 방향, 0이면 무관)
→ 학습셋에선 그럭저럭 양의 상관관계 있음
→ 테스트셋에선 거의 상관관계 없음 ==> 모델이 미래 데이터의 패턴을 전혀 못 따라감

Model Bias =  0.94
→ Long 시그널 비율 / 전체 시그널 수
→ 1.0이면 전부 Long, 0.0이면 전부 Short, 0.5면 균형
→ 여기서는 0.94 → 예측이 거의 Long(매수) 쪽에 치우쳐 있음

````
📌 모델은 계속 최적화하고 튜닝하는 대상. 최적화 과정에는 다음과 같은 여러 기법이 포함 될 수 있다.
1. 예측 변수(predictors) 선택이 매우 중요하다
   - 이 장에서는 과거 수익률(lagged returns) 을 예측 변수로 사용했음
   - 예측 변수는 경제적, 통계적 직관에 따라 선택 (자기 기준에 맞춰)
     - 금 가격 변동이 S&P 500 지수 변동을 설명하는 데 도움이 될 수 있음. 
     - 기술적 지표(technical indicators)를 예측 변수로 사용
2. 데이터 분할(train-test split)이 중요하다
   - 모델 성능 평가를 위해 학습 데이터와 테스트 데이터를 적절히 나누는 것이 매우 중요
   - 보통 **20% 테스트 / 80% 학습** 또는 **30% 테스트 / 70% 학습** 비율이 자주 사용.
3. 정규화(Regularization) 기법으로 편향 방지
   - 선형 회귀에서는 `릿지 회귀(Ridge Regression)` 와 `라쏘 회귀(Lasso Regression)` 같은 **정규화 방법을 사용**
   - **릿지 회귀:**
     - OLS(최소제곱법) 함수에 패널티 항을 추가하여, 큰 계수(coefficient)가 너무 커지는 것을 막는다.
     - 계수(기울기) 값이 너무 크지 않도록, `패널티를 통해 보정` 해주는 것.
   - **라쏘 회귀:**
     - 계수를 아예 0으로 만들 수 있어서, 특징 선택(feature selection) 효과도 낸다.
     - 일부 변수의 계수를 0으로 만들어버려서, 그 **변수는 모델에서 사실상 제외**

- **자기회귀 모델(autoregressive model)**
  - `종속 변수`(예측하려는 값)가 외부 데이터(exogenous data)가 아니라, `자신의 과거 값들에 의존해서 예측`
- **다중 선형 회귀 모델(multiple linear regression model)**
  - 매 시점마다 500개의 서로 다른 변수들(그리고 그에 해당하는 계수들)을 사용해 다음 값을 예측
- **단순 선형 회귀 모델(simple linear regression model)**
  -  단 하나의 독립 변수만을 사용해 종속 변수를 예측


> **선형 회귀의 장점**
> - 구현이 간단하고 학습시키기 쉽다. 
> - 메모리를 많이 차지하지 않는다.
> - 데이터 간에 선형적인 관계가 있을 때는 좋은 성능을 보인다.

> **선형 회귀의 단점**
> - 이상치(outlier)에 민감
> - 편향(bias)이 쉽게 생길 수 있다.
> - 데이터가 서로 독립적이라는 비현실적인 가정을 전제로 한다.

- 일부 **선형 회귀 모델은 데이터를 변환**(transform)하지 않는다.
  - 이런 경우, 예측 정확도가 매우 높고, 예측값이 실제값과 거의 똑같이 보일 수 있습니다.
  - 하지만 실제로는, 예측이 한 시점 뒤처져 있는(lag) 경우가 많습니다.
  - 즉, **각 시점의 예측값이 단순히 그 이전 시점의 실제값**일 뿐

#### 선형 회귀와 비정상 시계열 데이터의 문제점
- 비정상 시계열 데이터 + 선형 회귀 = 예측 실패 위험 높음
- 데이터 전처리(차분 등) 없이 쓰면 예측값이 의미 없이 마지막 값 복사가 됨
- 반드시 데이터를 정규화/변환하고 모델을 설계해야 진짜 예측이 가능함

📌 선형 회귀가 제대로 작동하지 않는 이유들
1. 예측력이 부족함 (Lack of Predictive Power)
 - 선형 회귀는 독립 변수와 종속 변수 사이의 선형 관계가 있다고 가정함.
 - 데이터가 비정상적이거나 선형 관계가 약하다면, 모델은 의미 있는 패턴을 잡지 못하고 순진한 예측(naive forecasting) 만 진행
2. 후행 지표만 사용하는 문제 (Lagging Indicators)
   - 금융 시장 가격은 자동상관(autocorrelation)이 강해서 **현재 가격이 이전 가격과 매우 유사한 경향**이 있음.
   - 그래서 모델이 단순히 이전 값만 예측하게 될 위험이 큼.

3. 특징 엔지니어링 부족 (Lack of Feature Engineering)
   - 선형 회귀 모델은 입력한 특성(피처)에 의존함.
   - 연된 값(lagged values)만 넣고, 금리, 거래량, 기술 지표 등 다른 관련 변수를 포함하지 않으면, **모델은 더 풍부한 예측을 하지 못함.**
4. 모델 자체가 단순함 (Model Complexity) 
   - 선형 회귀는 구조가 단순해서, 데이터 내에 있는 **복잡한 비선형 관계나 패턴을 잡지 못할 수 있음** 

### Support Vector Regression (서포트 벡터 머신)
- SVR
  - 머신러닝 기법의 한 종류
  - 예측할 값이 `숫자(연속형)`일 때 사용하는 **회귀용 알고리즘**
🔧 SVR의 작동 방식 요약
1. 기본 목표
  - 일반적인 회귀는 실제 값과 예측 값 사이의 오차를 최소화 - 어떤 숫자(연속된 값)를 예측하는 문제를 회귀 라고함
  - SVR은 대부분의 데이터가 들어가는 `ε(엡실론)이라는 마진 구간` 안에 예측이 들어가도록 하는 `최적의 경계(하이퍼플레인)`를 찾는 게 목적
> 너무 정확히 맞추려고 하기보다, 어느 정도 오차는 괜찮다고 보고, 그 안에 최대한 많이 맞추는 전략
2. 고차원 공간으로 보내기 (커널 트릭)
  - 현실 데이터는 직선(선형)으로 나누기 어려운 경우가 많음.
  - 그래서 **커널 함수**(Kernel Function)를 이용해 데이터를 고차원 공간으로 변형(매핑)
> 그렇게 하면 비선형 관계도 선형처럼 다룰 수 있게된다.

🧠 SVR의 핵심 개념
- **Epsilon-tube (ε-튜브)**
  - 예측값이 실제값과 어느 정도 차이나도 괜찮은 허용 구간. => 이 범위 안에 예측값이 들어가도록 만드는 것이 목표
  - 튜브 밖으로 나간 점들에 대해서는 오차를 최소화하도록 최적화 문제를 푼다. => 주로 쓰는 손실 함수는 **MSE (평균제곱오차)**
- **커널 함수**
  - 데이터를 고차원으로 보낼 때 쓰는 함수
  - 어떤 커널을 사용할지는 데이터의 특성과 문제의 종류에 따라 달라진다.
    - 선형 커널 (Linear) : 데이터가 선형적인 경우
    - 다항식 커널 (Polynomial) : 곡선처럼 휘어진 패턴이 있을 때
    - RBF (Radial Basis Function) : 비선형 데이터에 매우 자주 사용됨 => 제일 흔함
    - 시그모이드 커널 (Sigmoid) : 뉴런처럼 동작하는 형태

> Note
> - RBF 커널은 SVR에서 자주 쓰이는 인기 있는 커널
>   - 그 이유는 비선형 관계를 효과적으로 포착할 수 있기 때문
>   - 구체적인 형태에 대한 사전 지식이 없을 때 적합
> - RBF 커널은 입력 공간에서 특징 벡터들 간의 거리를 기반으로 유사도를 계산
>   - 감마(gamma)라는 파라미터가 사용되며, 이 감마는 각 학습 데이터가 모델에 미치는 영향력을 결정
>   - 감마 값이 클수록 모델은 개별 데이터 포인트에 더 집중하게 되어 잠재적으로 오류가 발생할 수 있다.

- **SVR**은 노이즈나 이상치가 있어도 `데이터의 패턴을 잘 잡아내는 강력한 회귀 기법`
- **비선형 관계를 다룰 때 특히 효과적**

````python
model = make_pipeline(StandardScaler(), SVR(kernel = 'rbf', C = 1, gamma = 0.04, epsilon = 0.01))
````
- **비선형 관계**
  - 입력 값(x)이 변할 때 출력 값(y)이 직선 형태로 변하지 않는 관계
- **규제 강도를 넣는 이유**
  - 규제 강도는 모델이 훈련 데이터에 너무 집착하지 않도록 제어해서, 새로운 데이터에도 잘 작동하도록 도와주는 안전장치.
- **어느 정도가 적당할까?**
  - `gamma`: 작게 시작해서 조금씩 키워보는 것이 좋음. 예: 0.01 → 0.04 → 0.1
    - | gamma 값                | 손전등 빛의 범위 | 의미                   | 결과                       |
      | ---------------------- | --------- | -------------------- | ------------------------ |
      | **작은 gamma (예: 0.01)** | 넓게 퍼짐     | **멀리 있는 점들까지 함께 고려** | 예측이 **부드럽고 일반적** (덜 민감함) |
      | **큰 gamma (예: 1.0)**   | 좁고 강하게 퍼짐 | **아주 가까운 점만 고려**     | 예측이 **날카롭고 복잡** (너무 민감함) |

  - `epsilon`: 데이터에 노이즈가 많으면 0.05 ~ 0.1, 정밀도가 중요하면 0.001 ~ 0.01

---

| 파라미터           | 의미                                                     |
| -------------- | ------------------------------------------------------ |
| `kernel='rbf'` | RBF(방사 기저 함수) 커널 사용. **비선형 관계**를 잘 포착                  |
| `C=1`          | 규제 강도. **오차에 얼마나 민감할지** 정함. (크면 과적합 위험, 작으면 일반화 잘됨)    |
| `gamma=0.04`   | 커널의 영향 범위. **작으면 부드럽게**, **크면 개별 점에 예민**함              |
| `epsilon=0.01` | 오차 허용 범위 (ε-tube). 이 안의 오차는 **무시**함. 작을수록 더 정밀하게 맞추려 함 |


- ![img.png](images/ch07/img_39.png)
````text
Accuracy Train = 57.81%
> 학습 데이터에서 모델의 예측 정확도가 약 57.8%라는 뜻이에요. 학습 데이터에서는 어느 정도 맞추고 있다는 의미

Accuracy Test = 49.67%
> 테스트 데이터(새로운 데이터)에서는 정확도가 약 49.7%로 떨어짐. 
> 즉, 실제로 새 데이터를 맞추는 성능은 학습 때보다 낮음

RMSE Train = 0.0060
> 학습 데이터에서 예측값과 실제값 사이의 평균 제곱근 오차가 작다는 뜻
> 숫자가 작을수록 예측이 실제에 가깝다는 의미.

RMSE Test = 0.0054
> 테스트 데이터에서도 오차가 약간 더 작게 나옴
> RMSE 기준으로는 학습과 테스트 성능 차이가 크지 않음

Correlation In-Sample Predicted/Train = 0.686
> 학습 데이터에서 예측값과 실제값의 상관관계가 0.686으로 비교적 높은 편
> 선형 관계가 어느 정도 포착됐다는 의미

Correlation Out-of-Sample Predicted/Test = 0.019
> 테스트 데이터에서는 예측값과 실제값의 상관관계가 거의 0에 가까워서, 새 데이터에선 예측이 거의 무작위 수준임을 보여줌.
> 모델이 테스트 데이터에 대해 거의 아무런 패턴도 포착하지 못했고, 예측값이 실제 결과와 상관이 없다는 뜻.

Model Bias = 1.02
> 모델 편향이 거의 1에 가까워서, 롱(상승)과 숏(하락) 신호가 균형을 이룬 상태임을 나타냄
````
#### SVR의 장점과 단점
- 장점:
  - 특징(feature)이 많고 복잡한 데이터셋에서도 잘 작동하며, 고차원 공간에서도 성능이 좋다.
  - 커널 함수를 사용해 입력 변수와 목표 변수 간의 비선형 관계도 잘 포착한다.
  - epsilon-튜브라는 방식을 사용해 이상치(outlier)에 강건하다. 
    - 대부분 데이터에 맞추면서 이상치의 영향을 줄인다.
- 단점:
  - 최적 성능을 위해 여러 하이퍼파라미터를 조정해야 하는데, 적절한 값을 찾기가 어렵고 시간이 많이 걸릴 수 있다.
  - 복잡한 커널을 쓰거나 데이터가 클 경우 계산 비용이 많이 든다.
  - 하이퍼파라미터 선택에 민감해서 잘못 설정하면 모델 성능이 크게 떨어질 수 있다.

### Stochastic Gradient Descent Regression

### Nearest Neighbors Regression

### Decision Tree Regression

### Random Forest Regression

### AdaBoost Regression

### XGBoost Regression

## Overfitting and Underfitting

## Summary