# Chapter 7. Machine Learning Models for Time Series Prediction (시계열 예측을 위한 머신러닝 모델)
- `머신러닝`
  - 인공지능(AI)의 하위 분야
  - 컴퓨터가 명시적으로 프로그래밍되지 않아도 `학습`하고 `예측`이나 **결정**을 내릴 수 있도록 하는 **알고리즘과 모델의 개발**에 초점을 맞춘다. ==> 따라서, 그래서 `학습`이라는 용어가 사용
  - 일반적으로 `대량의 데이터를 분석`하고 그 안에서 **패턴을 추출**함으로써 이루어진다.
## The Framework
- 프레임워크는 연구 전반의 과정을 조직화하는 중요한 요소(데이터 수집부터 성능 평가까지)
- 적절한 프레임워크를 갖추는 것이 중요한 이유 
  - 백테스트 전반에 **일관성을 보장**
  -  다양한 머신러닝 모델 간의 **비교**가 가능
- 프레임워크는 일반적으로 `다음과 같은 순서`로 진행
  1. **데이터 가져오기 및 전처리**
     - 예측 모델을 제대로 백테스트하고 평가하려면 충분한 양의 데이터를 가져와야 한다. 
  2. **학습-테스트 분할 수행**
     - 이과정은 데이터를 두 부분으로 나누는 것
       - 첫 번째 부분은 알고리즘을 학습시키는 데 사용(미래 값을 예측하기 위한 수학적 공식 추출)
       - 두 번째 부분은 알고리즘이 한 번도 본 적 없는 데이터에 대해 얼마나 잘 작동하는지를 테스트
  3. **알고리즘을 사용해 학습(피팅)하고 예측(테스트) 수행**
  4. **성능 평가 알고리즘 실행**
     - 모델이 과거 데이터를 얼마나 잘 예측했는지 이해하기 위해 평가를 수행

> **Note**  
> - 학습용 데이터셋은 **인샘플(in-sample)** 데이터 라고도 한다.
> - 테스트용 데이터셋은 **아웃샘플(out-of-sample)** 데이터 라고도 한다.

- `학습용(in-sample)` 데이터셋에서는 모델을 학습시켜 **예측 함수**(forecasting function)를 도출
- `테스트용(out-of-sample)` 데이터셋에서는 학습 데이터로 계산된 예측 함수를 적용해 그 **성능을 평가**

#### master_function.py
- 시계열 데이터를 전처리(변환), 네 개의 배열(또는 원한다면 데이터프레임)로 나눈다.
- **데이터를 나누기 전에 중요한 점**
  - 무엇을 예측할 것인지(종속 변수)
  - 무엇을 가지고 예측할 것인지(독립 변수)를 명확히 아는 것

- `배열 x_train` (모델이 공부하는 데이터)
  - 예측하고자 하는 변수의 변동을 설명해주는 **인샘플 독립 변수 집합**
  - **예측을 위한 입력값 또는 설명 변수(predictors)**
- `배열 y_train` (모델이 공부하는 데이터의 해답지)
  - 모델이 예측 함수를 학습할 수 있도록 돕는, **인샘플 종속 변수 집합**
  - **정답 데이터**
- `배열 x_test` (모델이 시험 보는 데이터)
  - 모델의 성능을 평가하기 위해 사용하는 **아웃샘플 독립 변수 집합**
  - **모델이 한 번도 본 적 없는 데이터**에 대한 테스트 입력값
- `배열 y_test` (시험의 해답지)
  - 모델이 예측해야 하는 실제 값들
  - 예측 결과와 비교할 **정답 값**

````text
master_function.py
> 최근 500개의 일별 EUR/USD 수익률이 현재 수익률에 대한 예측력
````
- 종속 변수 (예측값)
  - EUR/USD의 t+1 시점의 일간 수익률 (y 변수)
- 독립 변수 (입력값)
  - EUR/USD의 이전 500일간의 일간 수익률 (x 변수)

#### 더미 회귀 모델(dummy regression model)
- `더미 회귀`
  - 아주 간단한 규칙만을 사용하여 예측하는 비교용 머신러닝 알고리즘
  - 실제 예측 가치가 거의 없기 때문에 벤치마크(기준선)로만 사용
  - 더미 회귀의 `유용성`은, **만든 모델**이 이 모델보다 뛰어난지 여부를 확인하는 데 있다.
- 더미 회귀 과정
  1. 데이터를 가져온다.
  2. 데이터를 전처리하고 분할한다.
  3. 알고리즘을 학습시킨다. 
  4. 학습한 파라미터를 이용해 테스트 데이터에 대해 예측을 수행한다.  
     또한, 비교를 위해 학습 데이터에 대해서도 예측을 수행한다.
  5. 결과를 그래프로 나타내고 평가한다. 

- ![img.png](images/img_30.png)


- 더미 회귀(dummy regression)는 다음 전략 중 하나를 인자로 받을 수 있다
  - `mean (평균)`
    - 항상 학습 데이터(y_train)의 평균을 예측
  - `median (중앙값)`
    - 항상 학습 데이터의 중앙값을 예측
  - `quantile (분위수)`
    - 분위수 매개변수와 함께 제공될 경우, 학습 데이터의 지정된 분위수를 예측
  - `constant (상수)`
    - 사용자가 제공한 일정한 값을 항상 예측

- 앞서 본 코드에서 선택된 전략은 **mean**
  - 더미 회귀 전략을 사용할 경우 모든 예측 값은 단순히 학습 데이터(y_train)의 평균이 됩니다.
  - 더미 회귀 한계
    - 머신러닝 모델이 아니라 **기준선**(benchmark)으로만 사용된다.(더미 회귀는 학습하지 않고 평균 같은 단순한 값만 예측하기 때문)


- 모델이 잘 작동하는지 어떻게 알 수 있을까
  - `성능 평가`는 트레이딩과 알고리즘 개발에서 매우 중요한 개념
  - `성능 평가`를 통해 올바른 모델을 선택하고 실제 환경에 적용
- **성능 평가** 방법 크게 두 가지
  - `모델 평가` (Model Evaluation)
    - 알고리즘이 **예측을 얼마나 잘하는지 평가**
  - `트레이딩 평가`(Trading Evaluation)
    - 그 알고리즘을 사용해서 실제 매매했을 때 **금융 성과를 평가**

#### 모델 평가에서 자주 쓰는 지표
- **정확도(Accuracy)**
  - 공식 : 정확도(Accuracy) = (정확하게 맞춘 예측 수) ÷ (전체 예측 수) × 100
    - 예를 들어, 작년에 100번의 예측을 했고 그중 73번이 맞았다면, 정확도는 73%
  - `상승` 또는 `하락` **방향을 예측**
  - 금융권에서는 정확도를 **히트 비율(hit ratio)** 이라고도 한다.
- **예측의 정확도**는 예측값(y_predicted)이 실제값(y_test)과 얼마나 가까운지로도 평가 ==> **손실 함수(loss function) 사용**
- **손실 함수**(loss function)
  - 정의 : 예측값과 실제값 사이의 차이를 수학적으로 계산하는 함수
  1. **평균 절대 오차**(Mean Absolute Error, MAE)
     - `가장 기본적인 손실 함수` 
     - `MAE`는 **예측값과 실제값의 절대 차이들의 평균을 계산**
       - ![img.png](images/img_22.png)  
       - MAE는 예측값과 실제값 사이의 평균적인 거리(절대값 차이)를 나타내며, **MAE 값이 작을수록 모델의 예측이 더 정확하다는 의미**
  2. **평균 제곱 오차**(Mean Squared Error, MSE)
     - `MSE`는 **예측값과 실제값의 차이를 제곱하여 평균한 값** ==> 회귀 문제에서 자주 사용
     - MAE와 마찬가지로, **MSE가 낮을수록 모델이 더 정확하다는 의미** ==> but MSE는 오차를 제곱하기 때문에, **큰 오차에 더 민감**하게 반응한다.
     - 하지만, MSE는 제곱된 단위를 가지기 때문에 해석이 직관적이지 않을 수 있음. ==> 이를 해결 위해, RMSE(Root Mean Squared Error, 평균 제곱근 오차) 나옴
     - `RMSE`는 **MSE의 제곱근을 취함**으로써, 원래 목표 변수와 같은 단위로 되돌려 준다.
       - `RMSE`는 **예측값이 실제값으로부터 얼마나 떨어져 있는지를 측정** ==> 표준편차(standard deviation) 와 동등한 개념

> Note
> - 데이터에 극단적인 값이 포함되어 있거나, 오차의 절대적인 크기가 중요할 때는 **MAE를 사용하는 것이 적절**
> - 모델의 성능을 더 민감하게 개선하고 싶을 때 주로 **MSE를 사용하는 것이 적절**
>   - 모델의 정밀한 조정(tuning)을 원할 때는 MSE가 더 유용

- MAE, MSE, RMSE 같은 지표는 **낮을수록 좋다.**
  - MAE: 평균 절대 오차 (예측이 얼마나 벗어났는지 평균적으로)
  - MSE: 평균 제곱 오차 (큰 오차에 더 큰 패널티)
  - RMSE: MSE의 제곱근 (단위가 원래 값과 같아짐)
- `숫자가 낮을수록 예측이 정확`하다는 뜻, 하지만 이 수치 하나만으로 모델의 좋고 나쁨을 판단하기는 어렵다.
  - **비교 대상**(베이스라인)이 있어야 의미가 생긴다.
- 여러 모델을 비교하여, **수용 가능한 오차 기준**(threshold)을 정해서 그 기준을 넘는 모델은 탈락시킨다. 
- 모델의 **편향**(Bias)도 살펴봐야 한다
  - 예측 모델이 한 방향(예: 매수)만 많이 예측하는 경우, `편향된 모델`
  - ![img.png](images/img_23.png)
    - 어떤 모델이 올해 934개의 매수 포지션(long)과 899개의 매도 포지션(short)을 가졌다면, 모델 편향 지표는 1.038이며, 이는 허용 가능한 수준
    - 모델 편향이 0.0이라는 것은 매수 신호가 전혀 없었다는 뜻 (매도 없으면 → 0으로 나누기 → 편향 값 정의 불가)

- 수익성 지표
  - **순수익(Net Return)**: 수수료 등을 뺀 `최종 이익`을 의미
    - ![img.png](images/img_24.png)
  - **이익 계수(Profit Factor)**
    - Profit Factor = 총 이익 / 총 손실
      - 1.00보다 크면 → 수익성 있는 전략
      - 1.00보다 작으면 → 손실 나는 전략
  - **개별 거래(individual trades)**
    - 거래당 평균 수익(Average Gain per Trade): 과거 데이터 기반으로, `수익이 난 거래들의 평균 이익`
    - 거래당 평균 손실(Average Loss per Trade): 과거 데이터 기반으로, `손실이 난 거래들의 평균 손해`
    - ![img.png](images/img_25.png)
  - **위험(Risk)**
    - 최대 낙폭(Maximum Drawdown) : 투자 자산이나 포트폴리오의 가치가 최고점에서 최저점까지 얼마나 많이 하락했는지를 나타내는 지표
      - 이 지표는 자산이 `얼마나 크게 손실될 수 있는지를 보여주는 지표`
      - **하방 위험**(Downside Risk)을 평가할 때 자주 사용
      - ![img.png](images/img_26.png)
  - **샤프 비율(Sharpe Ratio)**
    - 위험을 얼마나 잘 감수하고 수익을 냈는지 보여주는 지표
      - ![img.png](images/img_27.png)
    - **샤프 비율 > 1.0**, 수익이 리스크보다 크다 = **좋은 전략**
    - **샤프 비율 < 1.0**, 리스크 대비 수익이 적다 = **비효율적 전략**
    - **샤프 비율 = 0**, 초과 수익 없음
    - **음수**, 무위험 수익률보다 못함 = **손실 전략**
#### 샤프비율 예시


  | 항목                  | 값                                             | 설명 |
| ------------------- |-----------------------------------------------| -- |
| **포트폴리오 수익률 (12%)** | 1년 동안 투자해서 총 **12% 수익**을 냈다는 뜻함.              |    |
| **무위험 수익률 (2%)**    | 아무런 위험 없이 얻을 수 있는 수익률. 예: **국채**나 **예금 이자율**. |    |
| **수익률 표준편차 (10%)**  | 이건 수익률의 변동성(위험도)를 뜻함. 숫자가 클수록 수익이 들쭉날쭉하다는 의미  |    |
- ![img.png](images/img_28.png)

> **리스크 프리 레이트 (Risk-Free Rate)**
> - 전혀 위험이 없는 투자에서 얻을 수 있는 이론적인 수익률
> - 다른 위험이 있는 투자들의 수익률을 평가할 때 **기준점(벤치마크) 역할**

- EUR-USD 모델 결과
  - ![img.png](images/img_29.png)
  - ![img.png](images/img_31.png)
  - **모델 편향(bias)이 0.0**이라는 것은, 이 모델이 `더미 회귀 모델(dummy regression model)`임을 뜻함
  - 여기서 편향(bias)이 0.0이라는 뜻은, 모델의 예측값이 **모두 하락(bearish) 방향임을 의미**

> **NOTE**
> - **적절한 백테스트**(backtest)를 위해서는 데이터를 반드시 **훈련 세트**(training set)와 테스트 **세트**(test set)로 나누어야 한다.
>   - 훈련 세트(training set)
>     - x_train: 예측에 사용될 독립 변수(입력값)
>     - y_train: 예측하려는 목표 변수(정답값)
>   - 테스트 세트(test set) : x_test로 예측을 수행, y_test 와 비교
>     - x_test: 모델이 한 번도 본 적 없는 입력값
>     - y_test: 그에 대한 실제 정답
> - 모델 학습(Fitting)
>   - 알고리즘이 **x_train**, **y_train**을 바탕으로 학습하는 과정
> - 모델 예측(Predicting)
>   - 학습한 모델이 **x_test**를 바탕으로 예측을 수행하는 과정
> - **알고리즘의 주요 목표**
>   - **높은 정확도(accuracy)** && **안정적이고 변동성이 낮은 수익률**을 달성

## Machine Learning Models
- 각 머신러닝 모델의 `강점`과 `약점`을 이해하는 것이 중요하다.
  - 예측 작업의 특성에 따라 **어떤 모델을 선택해야 할지 알 수 있기 때문**이다.

### Linear Regression (선형 회귀)
- **선형 회귀 알고리즘**
  - `목표`
    - **예측값과 실제값 사이의 제곱 오차의 합(Sum of Squared Differences)을 최소화하는 최적의 직선(line)을 찾는 것**
  - `최적화 기법`
    - 가장 많이 사용되는 방법은 **최소제곱법**(Ordinary Least Squares, OLS) 
    - 오차의 제곱합이 가장 작아지도록 **직선의 계수**(coefficient)를 조정
  - `학습 과정`
    - OLS를 이용해 학습 데이터로부터 독립 변수의 **최적 계수들을 계산**
    - 이 계수는 직선의 **기울기(slope)와 절편(intercept) 역할** ??
  - `최종 모델`
    - 예측값은 다음과 같은 선형 함수로 표현
    - ![img.png](images/img_32.png)
    - ![img.png](images/img_33.png)
      - 이 함수는 입력 변수들에 가중치를 곱한 값에 절편과 오차를 더해,**예측값**(예: 기대 수익)을 계산 ?? 

---
#### 1. 선형 회귀 도식
````yaml
[입력 데이터 X, Y]
   |
   v
[최소제곱법 (OLS)]
   → 예측값과 실제값의 오차²의 합을 최소화
   |
   v
[기울기 a, 절편 b 계산]
   |
   v
[선형 함수 생성]
   y = a * x + b + noise
   |
   v
[새로운 x 값으로 y 예측]
````
> 주어진 데이터에 가장 잘 맞는 직선을 그려서, 새로운 입력이 들어오면 해당 직선 위의 y값을 예측하는 구조
  
#### 2. 예시로 보는 선형 회귀
📊문제: 기온에 따라 아이스크림 판매량이 어떻게 변할까?

| 기온(℃) | 아이스크림 판매량(개) |
| ----- | ------------ |
| 20    | 100          |
| 22    | 120          |
| 25    | 150          |
| 27    | 170          |
| 30    | 200          |

🔍 선형 회귀의 역할
- 이런 데이터를 보고 선형 회귀는 다음과 같은 직선을 찾는다.
  - `판매량 = 5*기온+0`
    - 여기서 5는 기울기 → 기온이 1도 오르면, 판매량은 5개 증가
    - 0은 절편 → 단순화를 위해 여기선 0이라고 가정

❓ 예측해보기
- 기온이 28도일 때 아이스크림은 몇 개 팔릴까?
  - `판매량 = 5*28+0=140`
- 예측 결과: 약 140개 팔릴 것이라고 예측

> **선형 회귀**는 데이터의 흐름을 직선으로 잡아, **새로운 값이 들어왔을 때 결과를 예측**
---



✅ **제곱 오차의 합**이란?  
🎯 `목표`
- **직선이 데이터에 얼마나 잘 맞는지를 판단하는 기준**이 필요
- 그 기준이 바로 **오차의 제곱합**(Sum of Squared Errors)

📌 `오차(Error)`란?  
- 실제값 y가 있고, 우리가 만든 직선은 예측값(y`)
- 오차 = 실제값 - 예측값(즉, 점이 직선에서 얼마나 떨어져 있는지)

📌 `오차의 제곱합`이란?  
- 모든 데이터에 대해 오차를 각각 제곱해서 전부 더한 값
  - ![img.png](images/img_34.png)
- **이 값을 줄이는 게 선형 회귀의 목적** -> 이 값이 작을수록 직선이 데이터에 잘 맞는다는 뜻

✅ **최소제곱법**(OLS)이란?
- `오차의 제곱합이 가장 작아지는 기울기(a)와 절편(b)`를 **수학적으로 계산**하는 방법
  - 쉽게, 직선을 어떻게 그려야 오차가 가장 작을까? -> **수학적으로 푸는 방법이 바로 OLS**

🔍 아주 쉬운 예시
📊 데이터

| 공부시간(x) | 점수(y) |
| ------- | ----- |
| 1시간     | 50점   |
| 2시간     | 60점   |
| 3시간     | 70점   |

✅ 어떤 직선을 그릴까?
- y=a⋅x+b
  - 직선 후보 1 : y=10x+40
  - 직선 후보 2 : y=20x+30
- **OLS** 를 통해 가장 적합한 후보를 찾으면 된다.
---
✅ 예제) OLS 공식  

| 공부시간(x) | 점수(y) |
| ------- | ----- |
| 1시간     | 2점    |
| 2시간     | 4점    |
| 3시간     | 6점    |

- 1단계 : 먼저 평균을 구한다.
  - x 평균(𝑥̄`)= (1 + 2 + 3) ÷ 3 = 2
  - y 평균(y`) = (2 + 4 + 6) ÷ 3 = 4
- 2단계 : 기울기공식 
  - ![img.png](images/img_35.png)
  - (각 x가 평균에서 얼마나 떨어져있나) × (각 y가 평균에서 얼마나 떨어져있나) 를 다 더한 값 나누기
  - 각 x가 평균에서 얼마나 떨어져있나의 제곱을 다 더한 값
- 3단계: 계산

| i | $x_i$ | $y_i$ | $x_i - \bar{x}$ | $y_i - \bar{y}$ | 곱하기 $(x_i - \bar{x})(y_i - \bar{y})$ | 제곱 $(x_i - \bar{x})^2$ |
| - | ----- | ----- | --------------- | --------------- | ------------------------------------ | ---------------------- |
| 1 | 1     | 2     | 1 - 2 = -1      | 2 - 4 = -2      | (-1) × (-2) = 2                      | (-1)² = 1              |
| 2 | 2     | 4     | 2 - 2 = 0       | 4 - 4 = 0       | 0 × 0 = 0                            | 0² = 0                 |
| 3 | 3     | 6     | 3 - 2 = 1       | 6 - 4 = 2       | 1 × 2 = 2                            | 1² = 1                 |

- 4단계: 각 열을 모두 더하기
  - ![img.png](images/img_36.png)
- 5단계: 기울기 계산
  - `a = 4/2` **= 2**
- 6단계: 절편(b) 계산!
  - b = y`- a*x' = 4 -2*2 **= 0**

> **최종모델**
> - **y' = 2x+0**
---

- **예측값과 실제 목표값 간의 제곱 오차 합**(Sum of Squared Differences)을 최소화하는 최적의 직선을 찾는 방식
  - 선형 회귀 알고리즘에서 `가장 많이 사용되는 최적화 기법`은 **최소제곱법**(Ordinary Least Squares, OLS)입니다.
- 모델은 학습 데이터셋에 대해 OLS 방법을 사용하여 학습된다.
- 예측값과 실제 목표값 간의 제곱 오차 합을 최소화하는 **독립 변수의 최적 계수**(coefficient)를 추정한다.
- 이 계수들은 각각 최적 직선의 y절편과 기울기를 나타냅니다.
- 최종적으로 출력되는 것은, 계수로 가중치를 준 설명 변수들과 잡음(noise), 절편(intercept)을 고려하여 기대 수익(return)을 예측하는 선형 함수입니다.
### Support Vector Regression

### Stochastic Gradient Descent Regression

### Nearest Neighbors Regression

### Decision Tree Regression

### Random Forest Regression

### AdaBoost Regression

### XGBoost Regression

## Overfitting and Underfitting

## Summary